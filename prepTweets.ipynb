{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olavo\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "from fastparquet import write\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brands under analysis and respective search terms\n",
    "brands = dict(\n",
    "            Xbox=['xbox'],\n",
    "            Playstation=[\"ps1\", \"ps2\", \"ps3\", \"ps4\", \"ps5\", \"playstation\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of punctuation\n",
    "punctuation = [i for i in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                           strip_handles=True,\n",
    "                           reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the tags based on the solution proposed by \n",
    "# Shuchita Banthia on Stack Overflow\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with Language codes and languages\n",
    "languages = {\n",
    "            'und': 'undetected',\n",
    "            'af': 'Afrikaans', 'af-ZA': 'Afrikaans', 'ar': 'Arabic',\n",
    "            'ar-AE': 'Arabic', 'ar-BH': 'Arabic', 'ar-DZ': 'Arabic', \n",
    "            'ar-EG': 'Arabic', 'ar-IQ': 'Arabic', 'ar-JO': 'Arabic',\n",
    "            'ar-KW': 'Arabic', 'ar-LB': 'Arabic', 'ar-LY': 'Arabic',\n",
    "            'ar-MA': 'Arabic', 'ar-OM': 'Arabic', 'ar-QA': 'Arabic',\n",
    "            'ar-SA': 'Arabic', 'ar-SY': 'Arabic', 'ar-SY': 'Arabic',\n",
    "            'ar-YE': 'Arabic', 'az': 'Azeri', 'az-AZ': 'Azeri',\n",
    "            'art': 'Artificial languages', 'bn': 'Bengali', 'am': 'Amharic',\n",
    "            'be': 'Belarusian', 'be-BY': 'Belarusian', 'bg': 'Bulgarian',\n",
    "            'bg-BG': 'Bulgarian', 'bs-BA': 'Bosnian', 'ca': 'Catalan',\n",
    "            'ca-ES': 'Catalan', 'cs': 'Czech', 'cs-CZ': 'Czech', 'cy': 'Welsh',\n",
    "            'cy-GB': 'Welsh','da': 'Danish', 'da-DK': 'Danish', 'de': 'German',\n",
    "            'ckb': 'Central Kurdish',\n",
    "            'de-AT': 'German', 'de-CH': 'German', 'de-DE': 'German',\n",
    "            'de-LI': 'German', 'de-LU': 'German', 'dv': 'Divehi',\n",
    "            'dv-MV': 'Divehi', 'el': 'Greek', 'el-GR': 'Greek',\n",
    "            'en': 'English', 'en': 'English', 'en-AU': 'English',\n",
    "            'en-BZ': 'English', 'en-CA': 'English', 'en-CB': 'English',\n",
    "            'en-GB': 'English', 'en-IE': 'English', 'en-JM': 'English',\n",
    "            'en-NZ': 'English', 'en-PH': 'English', 'en-TT': 'English',\n",
    "            'en-US': 'English', 'en-ZA': 'English', 'en-ZW': 'English',\n",
    "            'eo': 'Esperanto', 'es': 'Spanish', 'es-AR': 'Spanish',\n",
    "            'es-BO': 'Spanish', 'es-CL': 'Spanish', 'es-CO': 'Spanish',\n",
    "            'es-CR': 'Spanish', 'es-DO': 'Spanish', 'es-EC': 'Spanish',\n",
    "            'es-ES': 'Spanish', 'es-GT': 'Spanish', 'es-HN': 'Spanish',\n",
    "            'es-MX': 'Spanish', 'es-NI': 'Spanish', 'es-PA': 'Spanish',\n",
    "            'es-PE': 'Spanish', 'es-PR': 'Spanish', 'es-PY': 'Spanish',\n",
    "            'es-SV': 'Spanish', 'es-UY': 'Spanish', 'es-VE': 'Spanish',\n",
    "            'et': 'Estonian', 'et-EE': 'Estonian', 'eu': 'Basque',\n",
    "            'eu-ES': 'Basque', 'fa': 'Farsi', 'fa-IR': 'Farsi',\n",
    "            'fi': 'Finnish', 'fi-FI': 'Finnish', 'fo': 'Faroese',\n",
    "            'fo-FR': 'Faroese', 'fr': 'French', 'fr-BE': 'French',\n",
    "            'fr-CA': 'French', 'fr-CH': 'French', 'fr-FR': 'French',\n",
    "            'fr-LU': 'French', 'fr-MC': 'French', 'gl': 'Galician',\n",
    "            'gl-ES': 'Galician', 'gu': 'Gujarati', 'gu-IN': 'Gujarati',\n",
    "            'he': 'Hebrew', 'he-IL': 'Hebrew', 'hi': 'Hindi', 'hi-IN': 'Hindi',\n",
    "            'hr': 'Croatian', 'hr-BA': 'Croatian', 'hr-HR': 'Croatian',\n",
    "            'ht': 'Haitian', 'hu': 'Hungarian', 'hu-HU': 'Hungarian', 'hy': '',\n",
    "            'ny': 'Armenian', 'ny-AM': 'Armenian', 'id': 'Indonesian',\n",
    "            'id-ID': 'Indonesian', 'is': 'Icelandic', 'is-IS': 'Icelandic',\n",
    "            'it': 'Italian', 'it-CH': 'Italian', 'it-IT': 'Italian',\n",
    "            'iw': 'Hebrew',\n",
    "            'ja': 'Japanese', 'ja-JP': 'Japanese', 'ka': 'Georgian',\n",
    "            'ka-GE': 'Georgian', 'ka': 'Georgian', 'ka-GE': 'Georgian',\n",
    "            'kk': 'Kazakh', 'kk-KZ': 'Kazakh', 'kn': 'Kannada',\n",
    "            'kn-IN': 'Kannada', 'in': 'Indonesian', 'ind': 'Indonesian',\n",
    "            'ko': 'Korean', 'km': 'Khmer',\n",
    "            'ko-KR': 'Korean', 'kok': 'Konkani', 'kok-IN': 'Konkani',\n",
    "            'ky': 'Kyrgyz', 'ky-KG': 'Kyrgyz', 'lt': 'Lithuanian',\n",
    "            'lt-LT': 'Lithuanian', 'lv': 'Latvian', 'lv-LV': 'Latvian',\n",
    "            'lo': 'Lao', 'ml': 'Malayalam',\n",
    "            'mi': 'Maori', 'mi-NZ': 'Maori', 'mk': 'FYRO Macedonian',\n",
    "            'mk-MK': 'FYRO Macedonian', 'mn': 'Mongolian', 'mn-MN': 'Mongolian',\n",
    "            'mr': 'Marathi', 'mr-IN': 'Marathi', 'ms': 'Malay',\n",
    "            'ms-BN': 'Malay', 'ms-MY': 'Malay', 'mt': 'Maltese',\n",
    "            'mt-MT': 'Maltese', 'nb': 'Norwegian', 'no': 'Norwegian',\n",
    "            'my': 'Malay', 'nb-NO': 'Norwegian', 'ne': 'Nepali',\n",
    "            'nl': 'Dutch', 'nl-BE': 'Dutch', 'nl-NL': 'Dutch', \n",
    "            'nn-NO': 'Norwegian', 'ns': 'Northern Sotho', 'or': 'Oriya',\n",
    "            'ns-ZA': 'Northern Sotho', 'pa': 'Punjabi', 'pa-IN': 'Punjabi',\n",
    "            'pl': 'Polish', 'pl-PL': 'Polish', 'ps': 'Pashto', \n",
    "            'ps-AR': 'Pashto', 'pt': 'Portuguese', 'pt-BR': 'Portuguese',\n",
    "            'pt-PT': 'Portuguese', 'qu': 'Quechua', 'qu-BO': 'Quechua',\n",
    "            'qu-EC': 'Quechua', 'qu-PE': 'Quechua', 'qme': '', 'qht': '',\n",
    "            'qam': '', 'ro': 'Romanian',\n",
    "            'qst': 'Relexified Portuguese-Based Creole',\n",
    "            'ro-RO': 'Romanian', 'ru': 'Russian', 'ru-RU': 'Russian',\n",
    "            'sa': 'Sanskrit', 'sa-IN': 'Sanskrit', 'se': 'Sami',\n",
    "            'se-FI': 'Sami', 'se-NO': 'Sami', 'se-SE': 'Sami', 'sk': 'Slovak',\n",
    "            'si': 'Sinhala', 'sd': 'Sindhi',\n",
    "            'sk-SK': 'Slovak', 'sl': 'Slovenian', 'sl-SI': 'Slovenian',\n",
    "            'sq': 'Albanian', 'sq-AL': 'Albanian', 'sr': 'Serbian', \n",
    "            'sr-BA': 'Serbian',\n",
    "            'sr-SP': 'Serbian', 'sv': 'Swedish', 'sv-FI': 'Swedish',\n",
    "            'sv-SE': 'Swedish', 'sw': 'Swahili', 'sw-KE': 'Swahili',\n",
    "            'syr': 'Syriac', 'syr-SY': 'Syriac', 'ta': 'Tamil',\n",
    "            'ta-IN': 'Tamil', 'te': 'Telugu', 'te-IN': 'Telugu', 'th': 'Thai',\n",
    "            'th-TH': 'Thai', 'tl': 'Tagalog', 'tl': 'Tagalog',\n",
    "            'tl-PH': 'Tagalog', 'tn': 'Tswana', 'tn--ZA': 'Tswana',\n",
    "            'tr': 'Turkish', 'tr-TR': 'Turkish', 'tt': 'Tatar',\n",
    "            'tt-RU': 'Tatar','ts': 'Tsonga', 'uk': 'Ukrainian',\n",
    "            'uk-UA': 'Ukrainian', 'ur': 'Urdu', 'ur-PK': 'Urdu', 'uz': 'Uzbek',\n",
    "            'uz-UZ': 'Uzbek', 'vi': 'Vietnamese', 'vi-VI': 'Vietnamese',\n",
    "            'xh': 'Xhosa', 'xh-ZA': 'Xhosa', 'zh': 'Chinese',\n",
    "            'zh-CN': 'Chinese', 'zh-HK': 'Chinese', 'zh-MO': 'Chinese',\n",
    "            'zh-SG': 'Chinese', 'zh-TW': 'Chinese', 'zu': 'Zulu',\n",
    "            'zu-ZA': 'Zulu', 'zxx': 'No linguistic content'          \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with stop words for every language in the data frame\n",
    "stop_words = dict()\n",
    "for l in list(set(languages.values())):\n",
    "    try:\n",
    "        stop_words[l] = stopwords.words(l)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for preprocessing tweets\n",
    "def processTweets(tweetsDf):\n",
    "        \n",
    "    # Create a dataframe\n",
    "    df = tweetsDf\n",
    "    \n",
    "    # Convert the column \"lang\" to string so we can filter it\n",
    "    df['lang'] = df.lang.astype('string')\n",
    "    \n",
    "    # Keeping the tweets written in Portuguese, Spanish, and English \n",
    "    # df = df[df.lang.isin(['pt','es', 'en'])]\n",
    "    \n",
    "    # Extracting user name and location from 'user'\n",
    "    df['name'] = df.user.apply(lambda x: x['displayname'])\n",
    "    df['location'] = df.user.apply(lambda x: x['location'])\n",
    "    \n",
    "    # Dropping column we don't plan using\n",
    "    df.drop(\n",
    "            [\n",
    "            '_type',\n",
    "            'url',\n",
    "            'renderedContent',\n",
    "            'user',\n",
    "            'retweetedTweet',\n",
    "            'quotedTweet',\n",
    "            'inReplyToTweetId',\n",
    "            'inReplyToUser',\n",
    "            'mentionedUsers',\n",
    "            'coordinates',\n",
    "            'place',\n",
    "            #'hashtags',\n",
    "            'cashtags',\n",
    "            'card',\n",
    "            'conversationId',\n",
    "            'sourceUrl',\n",
    "            'sourceLabel',\n",
    "            'links',\n",
    "            'source',\n",
    "            'media',\n",
    "            ],\n",
    "            axis=1,\n",
    "            inplace=True\n",
    "            )\n",
    "    \n",
    "    # Convert the hashtags into strings\n",
    "    df['hashtags'] = df.hashtags.astype('string')\n",
    "    \n",
    "    # Convert the language codes into strings\n",
    "    df['lang'] = df.lang.astype('string')\n",
    "    \n",
    "    # Convert language codes into standard language\n",
    "    df['language'] = df.lang.apply(lambda x: languages.get(x, 'Not Found'))\n",
    "    \n",
    "    # Convert the body of the tweet into strings\n",
    "    df['tweet'] = df.rawContent.astype('string')\n",
    "    \n",
    "    # Remove retweet text \"RT\"\n",
    "    df['clean_tweet'] = df.tweet.apply(lambda x: re.sub(r'^RT[\\s]+', '', x))\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    df['clean_tweet'] = df.clean_tweet.apply(\n",
    "                                        lambda x:\n",
    "                                            re.sub(\n",
    "                                                r'https?:\\/\\/.*[\\r\\n]*',\n",
    "                                                '',\n",
    "                                                x\n",
    "                                                )\n",
    "                                            )\n",
    "    \n",
    "    # Remove the hash # sign\n",
    "    df['clean_tweet'] = df.clean_tweet.apply(lambda x: re.sub(r'#', '', x))\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    df['token'] = df.clean_tweet.apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    # Lists of punctuation for each row\n",
    "    df['punctuation'] = [punctuation] * len(df.token)\n",
    "    \n",
    "    # Lists of stop words according with the row language\n",
    "    df['stop_words'] = df.language.apply(lambda x: stop_words.get(x, []))\n",
    "    \n",
    "    # Lists combining punctuation and stop words for each row\n",
    "    df['cleaner'] = df.stop_words + df.punctuation\n",
    "    \n",
    "    # An auxiliary column to help remove\n",
    "    # stop words and punctuation from the tokens\n",
    "    df['temp_ct'] = tuple(zip(df.token, df.cleaner))\n",
    "    \n",
    "    # Remove stop words and punctuation from the tokes\n",
    "    df['clean_tokens'] = df.temp_ct.apply(\n",
    "                                    lambda x:\n",
    "                                        list(set(x[0]).difference(set(x[1])))\n",
    "                                        )\n",
    "    \n",
    "    # Identify a Pos Tag (a characterization for the word) for each token\n",
    "    df['tags'] = df.token.apply(lambda x: pos_tag(x))\n",
    "    \n",
    "    # Add the lemma (root-meaning) for each token\n",
    "    df['lemmas'] = df.tags.apply(\n",
    "                            lambda x:\n",
    "                                [lematizer.lemmatize(i[0], tag_map[i[1][0]])\\\n",
    "                                    for i in x]\n",
    "                                )\n",
    "    \n",
    "    # An auxiliary column to help remove\n",
    "    # stop words and punctuation from the lemmas\n",
    "    df['temp_cl'] = tuple(zip(df.lemmas, df.cleaner))      \n",
    "    \n",
    "    # Remove stop words and punctuation from the tokes\n",
    "    df['clean_lemmas'] = df.temp_cl.apply(\n",
    "                                    lambda x:\n",
    "                                        list(set(x[0]).difference(set(x[1])))\n",
    "                                        )\n",
    "    df = df.drop(\n",
    "                columns=[\n",
    "                    'temp_cl', 'temp_ct', 'cleaner', 'punctuation',\n",
    "                    'stop_words',\n",
    "                    ]\n",
    "                )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTweets(brand):\n",
    "    \n",
    "    # Listing files for brand's search terms\n",
    "    files = []\n",
    "    for f in brands[brand]:\n",
    "        files.extend(glob.glob('.\\\\Data\\\\*{}.json'.format(f)))\n",
    "    \n",
    "    # Create the directory\n",
    "    path = f'.\\\\data\\\\{brand}'\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Determine if we should append the data frame\n",
    "    partitions = 0\n",
    "\n",
    "    # List the ids already processed\n",
    "    ids = []\n",
    "    \n",
    "    for f in files:\n",
    "              \n",
    "        # Data frame for the current file\n",
    "        df = pd.read_json(f, lines=True)\n",
    "        \n",
    "        # Remove the duplicate ids\n",
    "        df = df[~df.id.isin(ids)]\n",
    "        \n",
    "        # Process Tweets\n",
    "        df = processTweets(df)\n",
    "\n",
    "        # Add the brand to the data frame\n",
    "        df['Brand'] = brand\n",
    "        \n",
    "        # Save the data frame as a parquet file\n",
    "        df.to_parquet(\n",
    "                        '{}\\\\{}.parquet'.format(path, partitions),\n",
    "                        compression='GZIP',\n",
    "                        engine='pyarrow',\n",
    "                        )\n",
    "\n",
    "        # Update the need for appending a new partition\n",
    "        partitions += 1\n",
    "        \n",
    "        # Update the list of info already considered\n",
    "        ids.extend(df.id.values)\n",
    "    os.close(path)         \n",
    "    os.rename(path, f'.\\\\Data\\\\{brand}.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xbox\n",
      "Playstation\n"
     ]
    }
   ],
   "source": [
    "for k in brands:\n",
    "    mergeTweets(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "413f5215cca76903c75de145955d347394fa539ab263de9aeb004c7ff4086f11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
